{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"d:\\\\Dataset\\\\THUMOS14_ACMfeat\\\\THUMOS14-Features\\\\train\\\\train\"\n",
    "data_dir = \"d:\\\\Dataset\\\\ActivityNet1-3_ACMfeat\\\\features\\\\train\"\n",
    "\n",
    "list_files=os.listdir(data_dir)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\repos_fork\\\\ACM-Net'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(input_feature, sample_len):\n",
    "    \n",
    "    input_len = input_feature.shape[0]\n",
    "    assert sample_len > 0, \"WRONG SAMPLE_LEN {}, THIS PARAM MUST BE GREATER THAN 0.\".format(sample_len)\n",
    "    \n",
    "    if input_len < sample_len:\n",
    "        sample_idxs = np.random.choice(input_len, sample_len, replace=True)\n",
    "        sample_idxs = np.sort(sample_idxs)\n",
    "    elif input_len > sample_len:\n",
    "        sample_idxs = np.arange(sample_len) * input_len / sample_len\n",
    "        for i in range(sample_len-1):\n",
    "            sample_idxs[i] = np.random.choice(range(int(sample_idxs[i]), int(sample_idxs[i+1] + 1)))\n",
    "        sample_idxs[-1] = np.random.choice(np.arange(sample_idxs[-2], input_len))\n",
    "    else:\n",
    "        sample_idxs = np.arange(input_len)\n",
    "\n",
    "    return input_feature[sample_idxs.astype(int), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_sample(input_feature, sample_len):\n",
    "        \n",
    "    input_len = input_feature.shape[0]\n",
    "    assert sample_len > 0, \"WRONG SAMPLE_LEN {}, THIS PARAM MUST BE GREATER THAN 0.\".format(sample_len)\n",
    "\n",
    "    if input_len <= sample_len and input_len > 1: #shorter than T\n",
    "        sample_idxs = np.arange(input_len)# idxs follow vid\n",
    "    else: #larger than T\n",
    "        if input_len == 1:\n",
    "            sample_len = 2\n",
    "        sample_scale = input_len / sample_len\n",
    "        sample_idxs = np.arange(sample_len) * sample_scale\n",
    "        sample_idxs = np.floor(sample_idxs)\n",
    "\n",
    "    return input_feature[sample_idxs.astype(int), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 2048)\n",
      "(21, 2048)\n",
      "(143, 2048)\n",
      "(75, 2048)\n",
      "(152, 2048)\n",
      "(75, 2048)\n",
      "(180, 2048)\n",
      "(75, 2048)\n",
      "(102, 2048)\n",
      "(75, 2048)\n",
      "(101, 2048)\n",
      "(75, 2048)\n",
      "(336, 2048)\n",
      "(75, 2048)\n",
      "(316, 2048)\n",
      "(75, 2048)\n",
      "(394, 2048)\n",
      "(75, 2048)\n",
      "(134, 2048)\n",
      "(75, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_17388/2887013912.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return input_feature[sample_idxs.astype(np.int), :]\n"
     ]
    }
   ],
   "source": [
    "for fil in list_files:\n",
    "    data = np.load(os.path.join(data_dir,fil))\n",
    "    print(data.shape)\n",
    "    con_vid_feature=data\n",
    "    sample_segments_num=75\n",
    "    #con_vid_spd_feature = random_sample(con_vid_feature, sample_segments_num)\n",
    "    con_vid_spd_feature = uniform_sample(con_vid_feature, sample_segments_num)\n",
    "    print(con_vid_spd_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.  10.  12.  21.  26.  36.  44.  51.  53.  63.  65.  72.  76.  86.\n",
      "  91.  96. 105. 111. 116. 124. 133. 135. 141. 149. 155. 160. 167. 172.\n",
      " 182. 191. 191. 200. 207. 217. 222. 228. 229. 236. 248. 252. 256. 264.\n",
      " 273. 275. 284. 292. 293. 300. 309. 317. 325. 328. 335. 342. 345. 354.\n",
      " 360. 366. 371. 379. 387. 389. 401. 406. 412. 415. 421. 427. 434. 446.\n",
      " 453. 454. 459. 471. 475.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_17388/2136089776.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sample_idxs[i] = np.random.choice(range(np.int(sample_idxs[i]), np.int(sample_idxs[i+1] + 1)))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_17388/2136089776.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return input_feature[sample_idxs.astype(np.int), :]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 2048)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files in directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/mnt/d/Dataset/hacs_segments_features_I3D/features'\n",
    "import json\n",
    "with open(os.path.join(dir_path, \"gt.json\")) as gt_f:\n",
    "    gt_dict = json.load(gt_f)[\"database\"]\n",
    "\n",
    "vid_names=list(gt_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_train = []\n",
    "for v_name in vid_names:\n",
    "    if (gt_dict[v_name]['subset']=='training'):\n",
    "        vid_train.append(v_name)\n",
    "\n",
    "dir_path = \"/mnt/d/Dataset/hacs_segments_features_I3D/features/split_train.txt\"\n",
    "if os.path.exists(dir_path):\n",
    "    os.remove(dir_path)\n",
    "\n",
    "textfile = open(dir_path, \"w\")\n",
    "for element in vid_train:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_train = []\n",
    "for v_name in vid_names:\n",
    "    if(gt_dict[v_name]['subset']=='validation'):\n",
    "        vid_train.append(v_name)\n",
    "\n",
    "dir_path = \"/mnt/d/Dataset/hacs_segments_features_I3D/features/split_test.txt\"\n",
    "if os.path.exists(dir_path):\n",
    "    os.remove(dir_path)\n",
    "\n",
    "textfile = open(dir_path, \"w\")\n",
    "for element in vid_train:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing boundary generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_segments_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1762/3298119210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvideo_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcorrected_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtemporal_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_segments_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtemporal_gap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemporal_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgt_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_segments_num' is not defined"
     ]
    }
   ],
   "source": [
    "idx=3\n",
    "video_name = vid_names[idx]\n",
    "video_info = gt_dict[video_name]\n",
    "video_labels = video_info['annotations']\n",
    "video_second = float(video_info['duration'])\n",
    "corrected_second = video_second\n",
    "temporal_scale = sample_segments_num\n",
    "temporal_gap = 1. / temporal_scale\n",
    "gt_bbox = []\n",
    "gt_len_small = 3 * temporal_gap\n",
    "\n",
    "for j in range(len(video_labels)):\n",
    "    tmp_info = video_labels[j]\n",
    "    tmp_start = max(min(1, tmp_info['segment'][0] / corrected_second), 0)\n",
    "    tmp_end = max(min(1, tmp_info['segment'][1] / corrected_second), 0)\n",
    "    gt_bbox.append([tmp_start, tmp_end])\n",
    "\n",
    "gt_bbox = np.array(gt_bbox)\n",
    "gt_xmins = gt_bbox[:, 0]\n",
    "gt_xmaxs = gt_bbox[:, 1]\n",
    "gt_lens = gt_xmaxs - gt_xmins\n",
    "gt_len_small = 3 * temporal_gap  # np.maximum(self.temporal_gap, self.boundary_ratio * gt_lens)\n",
    "gt_start_bboxs = np.stack((gt_xmins - gt_len_small / 2, gt_xmins + gt_len_small / 2), axis=1)\n",
    "gt_end_bboxs = np.stack((gt_xmaxs - gt_len_small / 2, gt_xmaxs + gt_len_small / 2), axis=1)\n",
    "\n",
    "### ==== Additional ====\n",
    "gt_start_nonact = gt_bbox[:,1]+0.5*temporal_gap # get end border of end region\n",
    "gt_end_nonact = gt_bbox[:,0]-0.5*temporal_gap # get start border of start region\n",
    "\n",
    "ls_start = gt_start_nonact[:-1]\n",
    "ls_end = gt_end_nonact[1:]\n",
    "\n",
    "gt_nonact_bbox = [(0,gt_end_nonact[0])]\n",
    "#gt_nonact_bbox = \n",
    "gt_nonact_bbox.extend(list(zip(ls_start,ls_end)))\n",
    "gt_nonact_bbox.append((gt_start_nonact[-1],1))\n",
    "print(video_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 2048)\n",
      "(75, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_17388/2887013912.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return input_feature[sample_idxs.astype(np.int), :]\n"
     ]
    }
   ],
   "source": [
    "#for fil in list_files:\n",
    "data = np.load(os.path.join(data_dir,list_files[idx]))\n",
    "print(data.shape)\n",
    "con_vid_feature=data\n",
    "sample_segments_num=75\n",
    "#con_vid_spd_feature = random_sample(con_vid_feature, sample_segments_num)\n",
    "con_vid_spd_feature = uniform_sample(con_vid_feature, sample_segments_num)\n",
    "print(con_vid_spd_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10718447 0.1872233 ]\n",
      " [0.21133981 0.35825243]\n",
      " [0.38458253 0.47394175]\n",
      " [0.51996117 0.60547573]\n",
      " [0.63716505 0.66209709]]\n",
      "[0.19388997 0.3649191  0.48060842 0.6121424  0.66876376]\n",
      "[0.1005178  0.20467314 0.37791586 0.5132945  0.63049838]\n",
      "[0.19388997 0.3649191  0.48060842 0.6121424 ]\n",
      "[0.20467314 0.37791586 0.5132945  0.63049838]\n",
      "[(0, 0.10051779976900116), (0.19388996836462125, 0.2046731399793132), (0.36491909524240423, 0.37791585909870234), (0.4806084160800327, 0.5132945004011437), (0.6121423971733686, 0.63049838435145), (0.6687637566165583, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(gt_bbox)\n",
    "print(gt_start_nonact)\n",
    "print(gt_end_nonact)\n",
    "print(ls_start)\n",
    "print(ls_end)\n",
    "print(gt_nonact_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate each anchors and get segments that contain only non-action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_xmin = [temporal_gap * (i - 0.5) for i in range(temporal_scale)]\n",
    "anchor_xmax = [temporal_gap * (i + 0.5) for i in range(temporal_scale)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_xs = list(zip(anchor_xmin,anchor_xmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.006666666666666667, 0.006666666666666667)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_xs[0:10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign backgroun label to each segment\n",
    "list_tem_bak=[]\n",
    "for anchor_now in anchor_xs:\n",
    "    tem_bak = 0\n",
    "    for bound in gt_nonact_bbox:\n",
    "        if((max(0,anchor_now[0])>=bound[0]) and (min(anchor_now[1],1)<=bound[1])):\n",
    "            tem_bak = 1\n",
    "            break\n",
    "    list_tem_bak.append(tem_bak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "print(len(list_tem_bak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 2048)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_vid_spd_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_list = list(map(bool,list_tem_bak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "con_vid_spd_feature_bak=list(compress(con_vid_spd_feature, bool_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(con_vid_spd_feature_bak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jdx in range(len(anchor_xmin)):\n",
    "    match_score_start.append(np.max(\n",
    "        ioa_with_anchors(anchor_xmin[jdx], anchor_xmax[jdx], gt_start_bboxs[:, 0], gt_start_bboxs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03858951])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = data_list[index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(video_labels)):\n",
    "    tmp_info = video_labels[j]\n",
    "    tmp_start = max(min(1, tmp_info['segment'][0] / corrected_second), 0)\n",
    "    tmp_end = max(min(1, tmp_info['segment'][1] / corrected_second), 0)\n",
    "    gt_bbox.append([tmp_start, tmp_end])\n",
    "    tmp_gt_iou_map = iou_with_anchors(\n",
    "        self.match_map[:, 0], self.match_map[:, 1], tmp_start, tmp_end)\n",
    "    tmp_gt_iou_map = np.reshape(tmp_gt_iou_map,\n",
    "                                [self.temporal_scale, self.temporal_scale])\n",
    "    gt_iou_map.append(tmp_gt_iou_map)\n",
    "\n",
    "\n",
    "gt_bbox = np.array(gt_bbox)\n",
    "gt_xmins = gt_bbox[:, 0]\n",
    "gt_xmaxs = gt_bbox[:, 1] \n",
    "gt_start_bboxs = np.stack((gt_xmins - gt_len_small / 2, gt_xmins + gt_len_small / 2), axis=1)\n",
    "gt_end_bboxs = np.stack((gt_xmaxs - gt_len_small / 2, gt_xmaxs + gt_len_small / 2), axis=1)\n",
    "### ==== Additional ====\n",
    "gt_start_nonact = gt_end_bboxs[:,1] # get end border of end region\n",
    "gt_end_nonact = gt_start_bboxs[:,0] # get start border of start region\n",
    "\n",
    "ls_start = gt_start_nonact[1:]\n",
    "ls_end = gt_end_nonact[:-1]\n",
    "\n",
    "gt_nonact_bbox = list(zip(ls_start,ls_end))\n",
    "###\n",
    "return match_score_start, match_score_end, gt_iou_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "import torch \n",
    "import argparse \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset \n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dsource dataset code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CLASS_NAME = {\n",
    "    \"THUMOS\":['BaseballPitch', 'BasketballDunk', 'Billiards', 'CleanAndJerk',\n",
    "              'CliffDiving', 'CricketBowling', 'CricketShot', 'Diving',\n",
    "              'FrisbeeCatch', 'GolfSwing', 'HammerThrow', 'HighJump',\n",
    "              'JavelinThrow', 'LongJump', 'PoleVault', 'Shotput',\n",
    "              'SoccerPenalty', 'TennisSwing', 'ThrowDiscus', 'VolleyballSpiking'],\n",
    "\n",
    "    \"ActivityNet\":['Applying sunscreen', 'Archery', 'Arm wrestling', 'Assembling bicycle',\n",
    "                    'BMX', 'Baking cookies', 'Ballet', 'Bathing dog', 'Baton twirling',\n",
    "                    'Beach soccer', 'Beer pong', 'Belly dance', 'Blow-drying hair', 'Blowing leaves',\n",
    "                    'Braiding hair', 'Breakdancing', 'Brushing hair', 'Brushing teeth', 'Building sandcastles',\n",
    "                    'Bullfighting', 'Bungee jumping', 'Calf roping', 'Camel ride', 'Canoeing', 'Capoeira',\n",
    "                    'Carving jack-o-lanterns', 'Changing car wheel', 'Cheerleading', 'Chopping wood',\n",
    "                    'Clean and jerk', 'Cleaning shoes', 'Cleaning sink', 'Cleaning windows', 'Clipping cat claws',\n",
    "                    'Cricket', 'Croquet', 'Cumbia', 'Curling', 'Cutting the grass', 'Decorating the Christmas tree',\n",
    "                    'Disc dog', 'Discus throw', 'Dodgeball', 'Doing a powerbomb', 'Doing crunches', 'Doing fencing',\n",
    "                    'Doing karate', 'Doing kickboxing', 'Doing motocross', 'Doing nails', 'Doing step aerobics',\n",
    "                    'Drinking beer', 'Drinking coffee', 'Drum corps', 'Elliptical trainer', 'Fixing bicycle', 'Fixing the roof',\n",
    "                    'Fun sliding down', 'Futsal', 'Gargling mouthwash', 'Getting a haircut', 'Getting a piercing', 'Getting a tattoo',\n",
    "                    'Grooming dog', 'Grooming horse', 'Hammer throw', 'Hand car wash', 'Hand washing clothes', 'Hanging wallpaper',\n",
    "                    'Having an ice cream', 'High jump', 'Hitting a pinata', 'Hopscotch', 'Horseback riding', 'Hula hoop',\n",
    "                    'Hurling', 'Ice fishing', 'Installing carpet', 'Ironing clothes', 'Javelin throw', 'Kayaking', 'Kite flying',\n",
    "                    'Kneeling', 'Knitting', 'Laying tile', 'Layup drill in basketball', 'Long jump', 'Longboarding',\n",
    "                    'Making a cake', 'Making a lemonade', 'Making a sandwich', 'Making an omelette', 'Mixing drinks',\n",
    "                    'Mooping floor', 'Mowing the lawn', 'Paintball', 'Painting', 'Painting fence', 'Painting furniture',\n",
    "                    'Peeling potatoes', 'Ping-pong', 'Plastering', 'Plataform diving', 'Playing accordion', 'Playing badminton',\n",
    "                    'Playing bagpipes', 'Playing beach volleyball', 'Playing blackjack', 'Playing congas', 'Playing drums',\n",
    "                    'Playing field hockey', 'Playing flauta', 'Playing guitarra', 'Playing harmonica', 'Playing ice hockey',\n",
    "                    'Playing kickball', 'Playing lacrosse', 'Playing piano', 'Playing polo', 'Playing pool', 'Playing racquetball',\n",
    "                    'Playing rubik cube', 'Playing saxophone', 'Playing squash', 'Playing ten pins', 'Playing violin',\n",
    "                    'Playing water polo', 'Pole vault', 'Polishing forniture', 'Polishing shoes', 'Powerbocking', 'Preparing pasta',\n",
    "                    'Preparing salad', 'Putting in contact lenses', 'Putting on makeup', 'Putting on shoes', 'Rafting',\n",
    "                    'Raking leaves', 'Removing curlers', 'Removing ice from car', 'Riding bumper cars', 'River tubing',\n",
    "                    'Rock climbing', 'Rock-paper-scissors', 'Rollerblading', 'Roof shingle removal', 'Rope skipping',\n",
    "                    'Running a marathon', 'Sailing', 'Scuba diving', 'Sharpening knives', 'Shaving', 'Shaving legs',\n",
    "                    'Shot put', 'Shoveling snow', 'Shuffleboard', 'Skateboarding', 'Skiing', 'Slacklining',\n",
    "                    'Smoking a cigarette', 'Smoking hookah', 'Snatch', 'Snow tubing', 'Snowboarding', 'Spinning',\n",
    "                    'Spread mulch','Springboard diving', 'Starting a campfire', 'Sumo', 'Surfing', 'Swimming',\n",
    "                    'Swinging at the playground', 'Table soccer','Tai chi', 'Tango', 'Tennis serve with ball bouncing',\n",
    "                    'Throwing darts', 'Trimming branches or hedges', 'Triple jump', 'Tug of war', 'Tumbling', 'Using parallel bars',\n",
    "                    'Using the balance beam', 'Using the monkey bar', 'Using the pommel horse', 'Using the rowing machine',\n",
    "                    'Using uneven bars', 'Vacuuming floor', 'Volleyball', 'Wakeboarding', 'Walking the dog', 'Washing dishes',\n",
    "                    'Washing face', 'Washing hands', 'Waterskiing', 'Waxing skis', 'Welding', 'Windsurfing', 'Wrapping presents',\n",
    "                    'Zumba']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATASET_HYPER_PARAMS = {\n",
    "    \n",
    "    \"THUMOS\":{\n",
    "        \"dropout\":0.7,\n",
    "        \"lr\":1e-4,\n",
    "        \"weight_decay\":5e-5,\n",
    "        \"frames_per_sec\":25,\n",
    "        \"segment_frames_num\":16,\n",
    "        \"sample_segments_num\":750,\n",
    "        \n",
    "        \"feature_dim\":2048,\n",
    "        \"action_cls_num\":len(_CLASS_NAME[\"THUMOS\"]),\n",
    "        \"cls_threshold\":0.25,\n",
    "        \"test_upgrade_scale\":20,\n",
    "        # \"data_dir\":\"/DATA/W-TAL/THU14/\",\n",
    "        \"data_dir\":\"./data/THUMOS14/\",\n",
    "        \"test_gt_file\":\"./data/THUMOS14/gt.json\",\n",
    "        \"tiou_thresholds\":np.arange(0.1, 1.00, 0.10),\n",
    "        \"nms_thresh\":0.55,\n",
    "        \n",
    "        \"ins_topk_seg\":8,\n",
    "        \"con_topk_seg\":3,\n",
    "        \"bak_topk_seg\":3,\n",
    "        \n",
    "        \"loss_lamb_1\":2e-3,\n",
    "        \"loss_lamb_2\":5e-5,\n",
    "        \"loss_lamb_3\":2e-4,\n",
    "        \n",
    "    },\n",
    "    \n",
    "    \"ActivityNet\":{\n",
    "        \"dropout\":0.7,\n",
    "        \"lr\":1e-4,\n",
    "        \"weight_decay\":0.001,\n",
    "        \"frames_per_sec\":25,\n",
    "        \"segment_frames_num\":16,\n",
    "        \"sample_segments_num\":75,\n",
    "        \n",
    "        \"feature_dim\":2048,\n",
    "        \"action_cls_num\":len(_CLASS_NAME[\"ActivityNet\"]),\n",
    "        \"cls_threshold\":0.10,\n",
    "        \"test_upgrade_scale\":20,\n",
    "        \"data_dir\":\"/mnt/d/Dataset/ActivityNet1-3_ACMfeat/features\",\n",
    "        # \"data_dir\":\"/DATA/W-TAL/ActivityNet13/features\",\n",
    "        \"test_gt_file\":\"/mnt/d/Dataset/ActivityNet1-3_ACMfeat/gt.json\",\n",
    "        \"tiou_thresholds\":np.arange(0.50, 1.00, 0.05),\n",
    "        \"nms_thresh\":0.90,\n",
    "        \n",
    "        \"ins_topk_seg\":2,\n",
    "        \"con_topk_seg\":10,\n",
    "        \"bak_topk_seg\":10,\n",
    "        \n",
    "        \"loss_lamb_1\":5e-3,\n",
    "        \"loss_lamb_2\":5e-5,\n",
    "        \"loss_lamb_3\":0e-4,  \n",
    "    },\n",
    "\n",
    "    \"HACS\":{\n",
    "        \"dropout\":0.7,\n",
    "        \"lr\":1e-4,\n",
    "        \"weight_decay\":0.001,\n",
    "        \"frames_per_sec\":25,\n",
    "        \"segment_frames_num\":16, #number of frames clips as input\n",
    "        \"sample_segments_num\":75,\n",
    "        \n",
    "        \"feature_dim\":2048,\n",
    "        \"action_cls_num\":len(_CLASS_NAME[\"ActivityNet\"]),\n",
    "        \"cls_threshold\":0.10,\n",
    "        \"test_upgrade_scale\":20,\n",
    "        \"data_dir\":\"/mnt/d/Dataset/hacs_segments_features_I3D/hacs_segments_features\",\n",
    "        # \"data_dir\":\"/DATA/W-TAL/ActivityNet13/features\",\n",
    "        \"test_gt_file\":\"/mnt/d/Dataset/hacs_segments_features_I3D/HACS_segments_v1.1.1.json\",\n",
    "        \"tiou_thresholds\":np.arange(0.50, 1.00, 0.05),\n",
    "        \"nms_thresh\":0.90,\n",
    "        \n",
    "        \"ins_topk_seg\":2,\n",
    "        \"con_topk_seg\":10,\n",
    "        \"bak_topk_seg\":10,\n",
    "        \n",
    "        \"loss_lamb_1\":5e-3,\n",
    "        \"loss_lamb_2\":5e-5,\n",
    "        \"loss_lamb_3\":0e-4,     \n",
    "    },  \n",
    "\n",
    "    \"HACStoAct\":{\n",
    "        \"dropout\":0.7,\n",
    "        \"lr\":1e-4,\n",
    "        \"weight_decay\":0.001,\n",
    "        \"frames_per_sec\":25,\n",
    "        \"segment_frames_num\":16, #number of frames clips as input\n",
    "        \"sample_segments_num\":75,\n",
    "        \n",
    "        \"feature_dim\":2048,\n",
    "        \"action_cls_num\":len(_CLASS_NAME[\"ActivityNet\"]),\n",
    "        \"cls_threshold\":0.10,\n",
    "        \"test_upgrade_scale\":20,\n",
    "        \"src_data_dir\":\"/mnt/d/Dataset/hacs_segments_features_I3D/features\",\n",
    "        \"src_test_gt_file\":\"/mnt/d/Dataset/hacs_segments_features_I3D/gt.json\",\n",
    "        \"tgt_data_dir\":\"/mnt/d/Dataset/ActivityNet1-3_ACMfeat/features\",\n",
    "        \"tgt_test_gt_file\":\"/mnt/d/Dataset/ActivityNet1-3_ACMfeat/gt.json\",\n",
    "        \"tiou_thresholds\":np.arange(0.50, 1.00, 0.05),\n",
    "        \"nms_thresh\":0.90,\n",
    "        \n",
    "        \"ins_topk_seg\":2,\n",
    "        \"con_topk_seg\":10,\n",
    "        \"bak_topk_seg\":10,\n",
    "        \n",
    "        \"loss_lamb_1\":5e-3,\n",
    "        \"loss_lamb_2\":5e-5,\n",
    "        \"loss_lamb_3\":0e-4,  \n",
    "    }} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import sys\n",
    "#module_path = os.path.abspath(os.path.join('..'))\n",
    "#if module_path not in sys.path:\n",
    "#    sys.path.append(module_path)\n",
    "os.chdir('/mnt/d/repos_fork/ACM-Net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_args(dataset=None):\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\"This script is used for the weakly-supervised temporal aciton localization task.\")\n",
    "    \n",
    "    parser.add_argument(\"--checkpoint\", default=None, type=str)\n",
    "    parser.add_argument(\"--start_epoch\", default=0, type=int)\n",
    "    parser.add_argument(\"--gpu\", default='0', type=str)\n",
    "    parser.add_argument(\"--num_workers\", default=6, type=int)\n",
    "    parser.add_argument(\"--dataset\", default=\"THUMOS\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", default=16, type=int)\n",
    "    parser.add_argument(\"--epochs\", default=1000, type=int)\n",
    "    \n",
    "    parser.add_argument(\"--without_wandb\", action=\"store_true\")\n",
    "    parser.add_argument(\"--test\", action=\"store_true\")\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    if dataset is not None:\n",
    "        args.dataset = dataset\n",
    "    # Based on the selected dataset, we set dataset specific hyper-params. \n",
    "    if args.dataset == \"HACStoAct\":\n",
    "        args.class_name_lst = _CLASS_NAME['ActivityNet']\n",
    "    else:\n",
    "        args.class_name_lst = _CLASS_NAME[args.dataset]\n",
    "    args.action_cls_num = _DATASET_HYPER_PARAMS[args.dataset][\"action_cls_num\"]\n",
    "    \n",
    "    args.dropout = _DATASET_HYPER_PARAMS[args.dataset][\"dropout\"]\n",
    "    args.lr = _DATASET_HYPER_PARAMS[args.dataset][\"lr\"]\n",
    "    args.weight_decay = _DATASET_HYPER_PARAMS[args.dataset][\"weight_decay\"]\n",
    "    \n",
    "    args.frames_per_sec = _DATASET_HYPER_PARAMS[args.dataset][\"frames_per_sec\"]\n",
    "    args.segment_frames_num = _DATASET_HYPER_PARAMS[args.dataset][\"segment_frames_num\"]\n",
    "    args.sample_segments_num = _DATASET_HYPER_PARAMS[args.dataset][\"sample_segments_num\"]\n",
    "    args.feature_dim =  _DATASET_HYPER_PARAMS[args.dataset][\"feature_dim\"]\n",
    "    \n",
    "    args.cls_threshold = _DATASET_HYPER_PARAMS[args.dataset][\"cls_threshold\"]\n",
    "    args.tiou_thresholds = _DATASET_HYPER_PARAMS[args.dataset][\"tiou_thresholds\"]\n",
    "    #args.test_gt_file_path = _DATASET_HYPER_PARAMS[args.dataset][\"test_gt_file\"]\n",
    "    args.src_test_gt_file_path = _DATASET_HYPER_PARAMS[args.dataset][\"src_test_gt_file\"]\n",
    "    args.tgt_test_gt_file_path = _DATASET_HYPER_PARAMS[args.dataset][\"tgt_test_gt_file\"]\n",
    "    #args.data_dir = _DATASET_HYPER_PARAMS[args.dataset][\"data_dir\"]\n",
    "    args.src_data_dir = _DATASET_HYPER_PARAMS[args.dataset][\"src_data_dir\"]\n",
    "    args.tgt_data_dir = _DATASET_HYPER_PARAMS[args.dataset][\"tgt_data_dir\"]\n",
    "\n",
    "    args.test_upgrade_scale = _DATASET_HYPER_PARAMS[args.dataset][\"test_upgrade_scale\"]\n",
    "    args.nms_thresh = _DATASET_HYPER_PARAMS[args.dataset][\"nms_thresh\"]\n",
    "    \n",
    "    args.ins_topk_seg = _DATASET_HYPER_PARAMS[args.dataset][\"ins_topk_seg\"]\n",
    "    args.con_topk_seg = _DATASET_HYPER_PARAMS[args.dataset][\"con_topk_seg\"]\n",
    "    args.bak_topk_seg = _DATASET_HYPER_PARAMS[args.dataset][\"bak_topk_seg\"]\n",
    "    \n",
    "    args.loss_lamb_1  = _DATASET_HYPER_PARAMS[args.dataset][\"loss_lamb_1\"]\n",
    "    args.loss_lamb_2  = _DATASET_HYPER_PARAMS[args.dataset][\"loss_lamb_2\"]\n",
    "    args.loss_lamb_3  = _DATASET_HYPER_PARAMS[args.dataset][\"loss_lamb_3\"]\n",
    " \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#from config.model_config import build_args \n",
    "args = build_args(dataset=\"HACStoAct\")\n",
    "#src_dataset = build_src_dataset(args, phase=\"train\", sample=\"random\")\n",
    "#src_dataloader = DataLoader(src_dataset, batch_size=args.batch_size, shuffle=True, \n",
    "#                                    num_workers=args.num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(input_feature, sample_len):\n",
    "    \n",
    "    input_len = input_feature.shape[0]\n",
    "    assert sample_len > 0, \"WRONG SAMPLE_LEN {}, THIS PARAM MUST BE GREATER THAN 0.\".format(sample_len)\n",
    "    \n",
    "    if input_len < sample_len:\n",
    "        sample_idxs = np.random.choice(input_len, sample_len, replace=True)\n",
    "        sample_idxs = np.sort(sample_idxs)\n",
    "    elif input_len > sample_len:\n",
    "        sample_idxs = np.arange(sample_len) * input_len / sample_len\n",
    "        for i in range(sample_len-1):\n",
    "            sample_idxs[i] = np.random.choice(range(int(sample_idxs[i]), int(sample_idxs[i+1] + 1)))\n",
    "        sample_idxs[-1] = np.random.choice(np.arange(sample_idxs[-2], input_len))\n",
    "    else:\n",
    "        sample_idxs = np.arange(input_len)\n",
    "    \n",
    "    return input_feature[sample_idxs.astype(int), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_with_anchors(anchors_min, anchors_max, box_min, box_max):\n",
    "    \"\"\"Compute jaccard score between a box and the anchors.\n",
    "    \"\"\"\n",
    "    # calculate the overlap proportion between the anchor and all bbox for supervise signal,\n",
    "    # the length of the anchor is 0.01\n",
    "    len_anchors = anchors_max - anchors_min\n",
    "    int_xmin = np.maximum(anchors_min, box_min)\n",
    "    int_xmax = np.minimum(anchors_max, box_max)\n",
    "    inter_len = np.maximum(int_xmax - int_xmin, 0.)\n",
    "    union_len = len_anchors - inter_len + box_max - box_min\n",
    "    # print inter_len,union_len\n",
    "    jaccard = np.divide(inter_len, union_len)\n",
    "    return jaccard\n",
    "\n",
    "def ioa_with_anchors(anchors_min, anchors_max, box_min, box_max):\n",
    "    # calculate the overlap proportion between the anchor and all bbox for supervise signal,\n",
    "    # the length of the anchor is 0.01\n",
    "    len_anchors = anchors_max - anchors_min\n",
    "    int_xmin = np.maximum(anchors_min, box_min)\n",
    "    int_xmax = np.minimum(anchors_max, box_max)\n",
    "    inter_len = np.maximum(int_xmax - int_xmin, 0.)\n",
    "    scores = np.divide(inter_len, len_anchors)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceVidDataset(data.Dataset):\n",
    "\n",
    "        \n",
    "    def __init__(self, args, phase=\"train\", sample=\"random\"):\n",
    "        \n",
    "        self.temporal_scale = 75\n",
    "        self.temporal_gap = 1. / self.temporal_scale\n",
    "\n",
    "        self.phase = phase\n",
    "        self.sample = sample\n",
    "        self.data_dir = args.src_data_dir \n",
    "        self.sample_segments_num = args.sample_segments_num\n",
    "\n",
    "        with open(os.path.join(self.data_dir, \"gt.json\")) as gt_f:\n",
    "            self.gt_dict = json.load(gt_f)[\"database\"]\n",
    "            \n",
    "        if self.phase == \"train\":\n",
    "            self.feature_dir = os.path.join(self.data_dir, \"train\")\n",
    "            self.data_list = list(open(os.path.join(self.data_dir, \"split_train.txt\")))\n",
    "            self.data_list = [item.strip() for item in self.data_list]\n",
    "        else:\n",
    "            self.feature_dir = os.path.join(self.data_dir, \"test\")\n",
    "            self.data_list = list(open(os.path.join(self.data_dir, \"split_test.txt\")))\n",
    "            self.data_list = [item.strip() for item in self.data_list]\n",
    "        \n",
    "        self.class_name_lst = args.class_name_lst\n",
    "        self.action_class_idx_dict = {action_cls:idx for idx, action_cls in enumerate(self.class_name_lst)}\n",
    "        \n",
    "        self.action_class_num = args.action_cls_num\n",
    "\n",
    "        # Additional\n",
    "        \n",
    "        self.anchor_xmin = [self.temporal_gap * (i - 0.5) for i in range(self.temporal_scale)]\n",
    "        self.anchor_xmax = [self.temporal_gap * (i + 0.5) for i in range(self.temporal_scale)]\n",
    "        self.get_label()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        vid_name = self.data_list[idx]\n",
    "        vid_label = self.label_dict[vid_name]\n",
    "        vid_duration = self.gt_dict[vid_name][\"duration\"] #action duration in seconds\n",
    "        con_vid_feature = np.load(os.path.join(self.feature_dir, vid_name+\".npy\"))\n",
    "        \n",
    "        vid_len = con_vid_feature.shape[0] #number of frames\n",
    "        \n",
    "        if self.sample == \"random\":\n",
    "            con_vid_spd_feature = random_sample(con_vid_feature, self.sample_segments_num)\n",
    "        else:\n",
    "            con_vid_spd_feature = uniform_sample(con_vid_feature, self.sample_segments_num)\n",
    "        \n",
    "        con_vid_spd_feature = torch.as_tensor(con_vid_spd_feature.astype(np.float32)) #input_feature\n",
    "        \n",
    "        vid_label_t = torch.as_tensor(vid_label.astype(np.float32)) #video-level label\n",
    "        \n",
    "        #label_start, label_end, conf_score, bool_bkg_list = self._get_train_prop_label(idx,self.anchor_xmin,self.anchor_xmax)\n",
    "        label_start, label_end, sample_bkg_idx, sample_act_idx = self._get_train_prop_label(idx,self.anchor_xmin,self.anchor_xmax)\n",
    "\n",
    "        init_tensor = torch.zeros(self.sample_segments_num,2048)\n",
    "        if sample_bkg_idx.size!=0:\n",
    "            sample_bkg_feat = con_vid_spd_feature[sample_bkg_idx,:]\n",
    "            init_tensor[:len(sample_bkg_idx),:]=sample_bkg_feat\n",
    "\n",
    "        sample_bkg_feat = init_tensor\n",
    "\n",
    "\n",
    "        init_tensor = torch.zeros(self.sample_segments_num,2048)\n",
    "        if sample_act_idx.size!=0:\n",
    "            sample_act_feat = con_vid_spd_feature[sample_act_idx,:]\n",
    "            init_tensor[:len(sample_act_idx),:]=sample_act_feat\n",
    "            \n",
    "        sample_act_feat = init_tensor\n",
    "\n",
    "        if self.phase == \"train\":\n",
    "            #return con_vid_spd_feature, vid_label_t, conf_score, label_start, label_end, bool_bkg_list\n",
    "            return con_vid_spd_feature, vid_label_t, label_start, label_end, sample_bkg_feat, sample_act_feat\n",
    "        else:\n",
    "            return vid_name, con_vid_spd_feature, vid_label_t, vid_len, vid_duration\n",
    "\n",
    "    def get_label(self):\n",
    "        \n",
    "        self.label_dict = {}\n",
    "        for item_name in self.data_list:\n",
    "            \n",
    "            #print(type(self.gt_dict))\n",
    "            item_anns_list = self.gt_dict[item_name][\"annotations\"]\n",
    "            item_label = np.zeros(self.action_class_num)\n",
    "            for ann in item_anns_list:\n",
    "                ann_label = ann[\"label\"]\n",
    "                item_label[self.action_class_idx_dict[ann_label]] = 1.0\n",
    "            \n",
    "            self.label_dict[item_name] = item_label\n",
    "            #additional\n",
    "            #video_second = self.gt_dict[item_name][\"duration\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def _get_train_prop_label(self,index, anchor_xmin, anchor_xmax):\n",
    "        video_name = self.data_list[index]\n",
    "        video_info = self.gt_dict[video_name]\n",
    "        video_labels = video_info['annotations']\n",
    "        video_second = float(video_info['duration'])\n",
    "        corrected_second = video_second\n",
    "        temporal_scale = self.sample_segments_num\n",
    "        temporal_gap = 1. / temporal_scale\n",
    "\n",
    "        # change the measurement from second to percentage\n",
    "        # gt_bbox = active window in percentage\n",
    "        gt_bbox = []\n",
    "        gt_iou_map = []\n",
    "        for j in range(len(video_labels)):\n",
    "            tmp_info = video_labels[j]\n",
    "            tmp_start = max(min(1, tmp_info['segment'][0] / corrected_second), 0)\n",
    "            tmp_end = max(min(1, tmp_info['segment'][1] / corrected_second), 0)\n",
    "            gt_bbox.append([tmp_start, tmp_end])\n",
    "            #tmp_gt_iou_map = iou_with_anchors(\n",
    "            #    self.match_map[:, 0], self.match_map[:, 1], tmp_start, tmp_end)\n",
    "            #tmp_gt_iou_map = np.reshape(tmp_gt_iou_map,\n",
    "            #                            [self.temporal_scale, self.temporal_scale])\n",
    "            #gt_iou_map.append(tmp_gt_iou_map)\n",
    "\n",
    "        anchor_xmin = [temporal_gap * (i - 0.5) for i in range(temporal_scale)]\n",
    "        anchor_xmax = [temporal_gap * (i + 0.5) for i in range(temporal_scale)]\n",
    "\n",
    "        # generate R_s and R_e\n",
    "        # starting and ending region\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_xmins = gt_bbox[:, 0]\n",
    "        gt_xmaxs = gt_bbox[:, 1]\n",
    "        gt_lens = gt_xmaxs - gt_xmins\n",
    "        gt_len_small = 3 * self.temporal_gap  # np.maximum(self.temporal_gap, self.boundary_ratio * gt_lens)\n",
    "        gt_start_bboxs = np.stack((gt_xmins - gt_len_small / 2, gt_xmins + gt_len_small / 2), axis=1)\n",
    "        gt_end_bboxs = np.stack((gt_xmaxs - gt_len_small / 2, gt_xmaxs + gt_len_small / 2), axis=1)\n",
    "        #####################################################################################################\n",
    "\n",
    "        # calculate the ioa for all timestamp\n",
    "        match_score_start = []\n",
    "        for jdx in range(len(anchor_xmin)):\n",
    "            match_score_start.append(np.max(\n",
    "                ioa_with_anchors(anchor_xmin[jdx], anchor_xmax[jdx], gt_start_bboxs[:, 0], gt_start_bboxs[:, 1])))\n",
    "    \n",
    "        match_score_end = []\n",
    "        for jdx in range(len(anchor_xmin)):\n",
    "            match_score_end.append(np.max(\n",
    "                ioa_with_anchors(anchor_xmin[jdx], anchor_xmax[jdx], gt_end_bboxs[:, 0], gt_end_bboxs[:, 1])))\n",
    "        match_score_start = torch.Tensor(match_score_start)\n",
    "        match_score_end = torch.Tensor(match_score_end)\n",
    "\n",
    "        ### ==== Additional ====\n",
    "        gt_start_nonact = gt_bbox[:,1]+0.5*temporal_gap # get end border of end region\n",
    "        gt_end_nonact = gt_bbox[:,0]-0.5*temporal_gap # get start border of start region\n",
    "\n",
    "        ls_start = gt_start_nonact[1:]\n",
    "        ls_end = gt_end_nonact[:-1]\n",
    "\n",
    "        gt_nonact_bbox = [(0,gt_end_nonact[0])]\n",
    "        gt_nonact_bbox.extend(list(zip(ls_start,ls_end)))\n",
    "        gt_nonact_bbox.append((gt_start_nonact[-1],1))\n",
    "\n",
    "\n",
    "        anchor_xs = list(zip(anchor_xmin,anchor_xmax))\n",
    "        #Assign backgroun label to each segment\n",
    "        list_bkg_lbl=[]\n",
    "        for anchor_now in anchor_xs:\n",
    "            tem_bak = 0\n",
    "            for bound in gt_nonact_bbox:\n",
    "                if((max(0,anchor_now[0])>=bound[0]) and (min(anchor_now[1],1)<=bound[1])):\n",
    "                    tem_bak = 1\n",
    "                    break\n",
    "            list_bkg_lbl.append(tem_bak)\n",
    "\n",
    "        #bool_bkg_list = list(map(bool,list_tem_bak))\n",
    "        list_bkg_lbl = torch.Tensor(list_bkg_lbl)\n",
    "        ###\n",
    "        #return match_score_start, match_score_end, gt_iou_map, bool_bkg_list\n",
    "        src_bkg_list = list_bkg_lbl.nonzero()\n",
    "        #if(src_bkg_list.sum()>0):    \n",
    "        #   sample_bkgs_idx=np.random.choice(src_bkg_list.numpy().squeeze(), \n",
    "        #    np.minimum(10,len(src_bkg_list)), replace=False)\n",
    "        #else:\n",
    "        src_bkg_idx=src_bkg_list.numpy().squeeze()\n",
    "\n",
    "\n",
    "        src_act_list = (list_bkg_lbl == 0).nonzero()\n",
    "        #if(src_act_list.sum()>0):    \n",
    "        #    sample_act_idx=np.random.choice(src_act_list.numpy().squeeze(), \n",
    "        #    np.minimum(10,len(src_act_list)), replace=False)\n",
    "        #else:\n",
    "        src_act_idx=src_act_list.numpy().squeeze()\n",
    "\n",
    "        return match_score_start, match_score_end, src_bkg_idx, src_act_idx\n",
    "\n",
    "def build_src_dataset(args, phase=\"train\", sample=\"random\"):\n",
    "    \n",
    "    return SourceVidDataset(args, phase, sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataset = build_src_dataset(args, phase=\"train\", sample=\"random\") #random enable linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataloader = DataLoader(src_dataset, batch_size=args.batch_size, shuffle=True, \n",
    "                                    num_workers=args.num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_iter = iter(src_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input_feature, src_vid_label_t, label_start, label_end, src_bkg_feat,src_act_feat = data_source_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 75, 2048])\n",
      "tensor([136, 182,  69, 172, 101,  51,  25,  46, 141,  22, 142, 163, 139,  54,\n",
      "        120,  82])\n",
      "torch.Size([16, 75])\n",
      "torch.Size([16, 75, 2048])\n",
      "torch.Size([16, 75, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(src_input_feature.shape)\n",
    "print(torch.argmax(src_vid_label_t,axis=1))\n",
    "print(label_start.shape)\n",
    "print(src_bkg_feat.shape)\n",
    "print(src_act_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "num_segments = 75\n",
    "r_easy = 5\n",
    "k_easy = num_segments // r_easy\n",
    "print(df2.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 2048])\n"
     ]
    }
   ],
   "source": [
    "df = torch.reshape(src_bkg_feat,(np.prod(np.array(src_bkg_feat.shape[0:2])),2048))\n",
    "#print(df.shape)\n",
    "df2 = df[torch.nonzero(df.abs().sum(axis=1)).squeeze(),:]\n",
    "df2 = df2[np.random.choice(df2.shape[0],k_easy,replace=False),:]\n",
    "#print(df2.shape)\n",
    "#df2 = torch.reshape(df2[0:src_bkg_list.shape[0]*5,:],(src_bkg_list.shape[0],5,src_bkg_list.shape[2]))\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(src_act_list[:,:5,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 0],\n",
       "        [-1]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3,2,1]).nonzero()*-1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3,2,1]).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_tem_bak' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1762/944316316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_tem_bak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'list_tem_bak' is not defined"
     ]
    }
   ],
   "source": [
    "list_tem_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d40c56176e2e5d223f81aa507d0cb0cb6a4394951ecd81c21d7b9e2db29a8db5"
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
